{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_Build_a_processor_for_your_own_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShussainML/Bert-FARM-for-thesis-Work/blob/main/2_Build_a_processor_for_your_own_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBYoXQ0tcEVk"
      },
      "source": [
        "# FARM: Use your own dataset\n",
        "\n",
        "In [Tutorial 1](https://colab.research.google.com/drive/130_7dgVC3VdLBPhiEkGULHmqSlflhmVM#scrollTo=tPltDefXjSiJ) you already learned about the major building blocks.   \n",
        "In this tutorial, you will see how to use FARM with your own dataset. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5UBYG__boli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fbb71b8-e3f6-41ff-8dbb-53a538ff4a2d"
      },
      "source": [
        "# Install FARM\n",
        "!pip install farm==0.5.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting farm==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/e4/2f47c850732a1d729e74add867e967f058370f29a313da05dc871ff8465e/farm-0.5.0-py3-none-any.whl (207kB)\n",
            "\r\u001b[K     |█▋                              | 10kB 26.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 20kB 31.1MB/s eta 0:00:01\r\u001b[K     |████▊                           | 30kB 35.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 40kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 51kB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 61kB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 71kB 25.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 81kB 23.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 92kB 24.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 102kB 22.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 112kB 22.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 122kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 133kB 22.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 143kB 22.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 153kB 22.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 163kB 22.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 174kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 184kB 22.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 194kB 22.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 204kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 215kB 22.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from farm==0.5.0) (0.3.3)\n",
            "Collecting flask-cors\n",
            "  Downloading https://files.pythonhosted.org/packages/69/7f/d0aeaaafb5c3c76c8d2141dbe2d4f6dca5d6c31872d4e5349768c1958abc/Flask_Cors-3.0.9-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from farm==0.5.0) (50.3.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.6/dist-packages (from farm==0.5.0) (0.35.1)\n",
            "Collecting transformers==3.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 41.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from farm==0.5.0) (0.0)\n",
            "Collecting seqeval==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Collecting mlflow==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/ec/8c9448968d4662e8354b9c3a62e635f8929ed507a45af3d9fdb84be51270/mlflow-1.0.0-py3-none-any.whl (47.7MB)\n",
            "\u001b[K     |████████████████████████████████| 47.7MB 66kB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from farm==0.5.0) (5.4.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from farm==0.5.0) (2.23.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.6/dist-packages (from farm==0.5.0) (1.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from farm==0.5.0) (4.41.1)\n",
            "Collecting dotmap==1.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/eb/ee5f0358a9e0ede90308d8f34e697e122f191c2702dc4f614eca7770b1eb/dotmap-1.3.0-py3-none-any.whl\n",
            "Collecting Werkzeug==0.16.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/e4/a859d2fe516f466642fa5c6054fd9646271f9da26b0cac0d2f37fc858c8f/Werkzeug-0.16.1-py2.py3-none-any.whl (327kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 50.6MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/31/4d4861a90d66c287a348fd17eaefefcdc2e859951cab9884b555923f046d/boto3-1.16.23-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 63.9MB/s \n",
            "\u001b[?25hCollecting torch<1.7,>1.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/53/914885a93a44b96c0dd1c36f36ff10afe341f091230aad68f7228d61db1e/torch-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8MB 25kB/s \n",
            "\u001b[?25hCollecting flask-restplus\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/a6/b17c848771f96ad039ad9e3ea275e842a16c39c4f3eb9f60ee330b20b6c2/flask_restplus-0.13.0-py2.py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 36.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.6/dist-packages (from farm==0.5.0) (1.4.1)\n",
            "Requirement already satisfied: Six in /usr/local/lib/python3.6/dist-packages (from flask-cors->farm==0.5.0) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1->farm==0.5.0) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1->farm==0.5.0) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1->farm==0.5.0) (0.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 47.2MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 47.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1->farm==0.5.0) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1->farm==0.5.0) (2019.12.20)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 53.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->farm==0.5.0) (0.22.2.post1)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval==0.0.12->farm==0.5.0) (2.4.3)\n",
            "Collecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/aa/c261dfd7f4ba6ce4701846a2689a46e2a172e012171de4378fc2926e3bf0/alembic-1.4.3-py2.py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 56.8MB/s \n",
            "\u001b[?25hCollecting simplejson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/96/1e6b19045375890068d7342cbe280dd64ae73fd90b9735b5efb8d1e044a1/simplejson-3.17.2-cp36-cp36m-manylinux2010_x86_64.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 64.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.5.0) (3.13)\n",
            "Collecting docker>=3.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/b7/eb7b7138bb5e6d28cf84fa586fe594619ca097b6207caa5f2ebe0c66a4ed/docker-4.4.0-py2.py3-none-any.whl (146kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 64.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.5.0) (3.12.4)\n",
            "Collecting gitpython>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/d1/a7f8fe3df258549b303415157328bfcc63e9b11d06a7ad7a3327f3d32606/GitPython-3.1.11-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 63.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlparse in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.5.0) (0.4.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.5.0) (2.8.1)\n",
            "Collecting querystring-parser\n",
            "  Downloading https://files.pythonhosted.org/packages/88/6b/572b2590fd55114118bf08bde63c0a421dcc82d593700f3e2ad89908a8a9/querystring_parser-1.2.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.5.0) (7.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.1.4)\n",
            "Collecting databricks-cli>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/88/ae1f78cf582b707c605c77df49b4c8786a4465edc51adb25d2f98ef4c4de/databricks-cli-0.14.1.tar.gz (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.5.0) (0.3)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.3.20)\n",
            "Collecting gunicorn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/ca/926f7cd3a2014b16870086b2d0fdc84a9e49473c68a8dff8b57f7c156f43/gunicorn-20.0.4-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->farm==0.5.0) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->farm==0.5.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->farm==0.5.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->farm==0.5.0) (3.0.4)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask->farm==0.5.0) (2.11.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask->farm==0.5.0) (1.1.0)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 12.5MB/s \n",
            "\u001b[?25hCollecting botocore<1.20.0,>=1.19.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/49/c8c99477416fdebb59078bda624acc5b3c7008f891c60d56d6ff1570d83e/botocore-1.19.23-py2.py3-none-any.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 50.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<1.7,>1.5->farm==0.5.0) (0.16.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from flask-restplus->farm==0.5.0) (2.6.0)\n",
            "Collecting aniso8601>=0.82\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/e4/787e104b58eadc1a710738d4e418d7e599e4e778e52cb8e5d5ef6ddd5833/aniso8601-8.0.0-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from flask-restplus->farm==0.5.0) (2018.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.3.1->farm==0.5.0) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3.1->farm==0.5.0) (0.17.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval==0.0.12->farm==0.5.0) (2.10.0)\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 13.1MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Collecting websocket-client>=0.32.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 52.7MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 12.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from databricks-cli>=0.8.0->mlflow==1.0.0->farm==0.5.0) (0.8.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask->farm==0.5.0) (1.1.1)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: seqeval, sacremoses, databricks-cli\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7423 sha256=e090d30fbabbf84cbc1264e18b8b1218d9ce127b20e3455881cf082c3802a15e\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=7853adb76c008da9d03837d0dfca5f3b8a91ac7c973e02dfb15fb9d0dde71029\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.14.1-cp36-none-any.whl size=100579 sha256=5d4cc65f790c39afa2a3100010eaba253b4806b3a56c8a29597e38c191f977c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/91/ac/5d417ee5ccbb76c8cca096cf4cfb9ed9d49d889d1d1ca0fc39\n",
            "Successfully built seqeval sacremoses databricks-cli\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: botocore 1.19.23 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: flask-cors, sacremoses, tokenizers, sentencepiece, transformers, seqeval, Mako, python-editor, alembic, simplejson, websocket-client, docker, smmap, gitdb, gitpython, querystring-parser, databricks-cli, gunicorn, mlflow, dotmap, Werkzeug, jmespath, botocore, s3transfer, boto3, torch, aniso8601, flask-restplus, farm\n",
            "  Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "Successfully installed Mako-1.1.3 Werkzeug-0.16.1 alembic-1.4.3 aniso8601-8.0.0 boto3-1.16.23 botocore-1.19.23 databricks-cli-0.14.1 docker-4.4.0 dotmap-1.3.0 farm-0.5.0 flask-cors-3.0.9 flask-restplus-0.13.0 gitdb-4.0.5 gitpython-3.1.11 gunicorn-20.0.4 jmespath-0.10.0 mlflow-1.0.0 python-editor-1.0.4 querystring-parser-1.2.4 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.94 seqeval-0.0.12 simplejson-3.17.2 smmap-3.0.4 tokenizers-0.8.1rc2 torch-1.6.0 transformers-3.3.1 websocket-client-0.57.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVqntSeur3ew"
      },
      "source": [
        "# 1) How a Processor works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMuNISLVctJI"
      },
      "source": [
        "<h2>Architecture</h2>\n",
        "The Processor converts a <b>raw input (e.g File) into a Pytorch dataset</b>.   \n",
        "For using an own dataset we need to adjust this Processor.\n",
        "<img src=\"https://raw.githubusercontent.com/deepset-ai/FARM/master/docs/img/data_silo_no_bg.jpg\", height=400 >\n",
        "\n",
        "\n",
        "## Main Conversion Stages \n",
        "1. Read from file / raw input \n",
        "2. Create samples\n",
        "3. Featurize samples\n",
        "4. Create PyTorch Dataset\n",
        "\n",
        "## Functions to implement\n",
        "\n",
        "1. _file_to_dicts()\n",
        "2. _dict_to_samples()\n",
        "3. _sample_to_features()  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qsLLsrGksoS"
      },
      "source": [
        "## Example: TextClassificationProcessor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3GH0952konH"
      },
      "source": [
        "from farm.data_handler.processor import *\n",
        "from farm.data_handler.samples import Sample\n",
        "from farm.modeling.tokenization import Tokenizer, tokenize_with_metadata\n",
        "\n",
        "import os\n",
        "\n",
        "# FARM has a built-in processor for Text Classification \n",
        "# -> farm.data_handler.processor.TextClassificationProcessor\n",
        "# That's how it looks like internally:\n",
        "class TextClassificationProcessor(Processor):\n",
        "    \"\"\"\n",
        "    Used to handle the text classification datasets that come in tabular format (CSV, TSV, etc.)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        max_seq_len,\n",
        "        data_dir,\n",
        "        label_list=None,\n",
        "        metric=None,\n",
        "        train_filename=\"train.tsv\",\n",
        "        dev_filename=None,\n",
        "        test_filename=\"test.tsv\",\n",
        "        dev_split=0.1,\n",
        "        delimiter=\"\\t\",\n",
        "        quote_char=\"'\",\n",
        "        skiprows=None,\n",
        "        label_column_name=\"label\",\n",
        "        multilabel=False,\n",
        "        header=0,\n",
        "        proxies=None,\n",
        "        max_samples=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        #TODO If an arg is misspelt, e.g. metrics, it will be swallowed silently by kwargs\n",
        "\n",
        "        # Custom processor attributes\n",
        "        self.delimiter = delimiter\n",
        "        self.quote_char = quote_char\n",
        "        self.skiprows = skiprows\n",
        "        self.header = header\n",
        "        self.max_samples = max_samples\n",
        "\n",
        "        # Init the parent processor class\n",
        "        super(TextClassificationProcessor, self).__init__(\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_len=max_seq_len,\n",
        "            train_filename=train_filename,\n",
        "            dev_filename=dev_filename,\n",
        "            test_filename=test_filename,\n",
        "            dev_split=dev_split,\n",
        "            data_dir=data_dir,\n",
        "            tasks={},\n",
        "            proxies=proxies,\n",
        "\n",
        "        )\n",
        "        # A task defines which labels to extract for a certain prediction head and which metric to use for eval.\n",
        "        # This becomes important for multitask learning, where we might have multiple text classification tasks.\n",
        "        if metric and label_list:\n",
        "            if multilabel:\n",
        "                task_type = \"multilabel_classification\"\n",
        "            else:\n",
        "                task_type = \"classification\"\n",
        "            self.add_task(name=\"text_classification\",\n",
        "                          metric=metric,\n",
        "                          label_list=label_list,\n",
        "                          label_column_name=label_column_name,\n",
        "                          task_type=task_type)\n",
        "        else:\n",
        "            logger.info(\"Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for \"\n",
        "                        \"using the default task or add a custom task later via processor.add_task()\")\n",
        "\n",
        "    # 1) Read from file to dicts\n",
        "    def file_to_dicts(self, file: str) -> [dict]:\n",
        "        column_mapping = {task[\"label_column_name\"]: task[\"label_name\"] for task in self.tasks.values()}\n",
        "        dicts = read_tsv(\n",
        "            filename=file,\n",
        "            delimiter=self.delimiter,\n",
        "            skiprows=self.skiprows,\n",
        "            quotechar=self.quote_char,\n",
        "            rename_columns=column_mapping,\n",
        "            header=self.header,\n",
        "            proxies=self.proxies,\n",
        "            max_samples=self.max_samples\n",
        "            )\n",
        "\n",
        "        return dicts\n",
        "\n",
        "    # 2) Convert one dict to tokenized sample(s)\n",
        "    def _dict_to_samples(self, dictionary: dict, **kwargs) -> [Sample]:\n",
        "        # this tokenization also stores offsets and a start_of_word mask\n",
        "        text = dictionary[\"text\"]\n",
        "        tokenized = tokenize_with_metadata(text, self.tokenizer)\n",
        "        if len(tokenized[\"tokens\"]) == 0:\n",
        "            logger.warning(f\"The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: {text}\")\n",
        "            return []\n",
        "        # truncate tokens, offsets and start_of_word to max_seq_len that can be handled by the model\n",
        "        for seq_name in tokenized.keys():\n",
        "            tokenized[seq_name], _, _ = truncate_sequences(seq_a=tokenized[seq_name], seq_b=None, tokenizer=self.tokenizer,\n",
        "                                                max_seq_len=self.max_seq_len)\n",
        "        return [Sample(id=None, clear_text=dictionary, tokenized=tokenized)]\n",
        "\n",
        "    # 3) Convert one sample to features\n",
        "    def _sample_to_features(self, sample) -> dict:\n",
        "        features = sample_to_features_text(\n",
        "            sample=sample,\n",
        "            tasks=self.tasks,\n",
        "            max_seq_len=self.max_seq_len,\n",
        "            tokenizer=self.tokenizer,\n",
        "        )\n",
        "        return features\n",
        "      \n",
        "      \n",
        "# Helper\n",
        "def read_tsv(filename, rename_columns, quotechar='\"', delimiter=\"\\t\", skiprows=None, header=0, proxies=None, max_samples=None):\n",
        "    \"\"\"Reads a tab separated value file. Tries to download the data if filename is not found\"\"\"\n",
        "    \n",
        "    # get remote dataset if needed\n",
        "    if not (os.path.exists(filename)):\n",
        "        logger.info(f\" Couldn't find {filename} locally. Trying to download ...\")\n",
        "        _download_extract_downstream_data(filename)\n",
        "    \n",
        "    # read file into df\n",
        "    df = pd.read_csv(\n",
        "        filename,\n",
        "        sep=delimiter,\n",
        "        encoding=\"utf-8\",\n",
        "        quotechar=quotechar,\n",
        "        dtype=str,\n",
        "        skiprows=skiprows,\n",
        "        header=header\n",
        "    )\n",
        "\n",
        "    # let's rename our target columns to the default names FARM expects: \n",
        "    # \"text\": contains the text\n",
        "    # \"text_classification_label\": contains a label for text classification\n",
        "    columns = [\"text\"] + list(rename_columns.keys())\n",
        "    df = df[columns]\n",
        "    for source_column, label_name in rename_columns.items():\n",
        "        df[label_name] = df[source_column]\n",
        "        df.drop(columns=[source_column], inplace=True)\n",
        "    \n",
        "    if \"unused\" in df.columns:\n",
        "        df.drop(columns=[\"unused\"], inplace=True)\n",
        "    raw_dict = df.to_dict(orient=\"records\")\n",
        "    return raw_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkM9a5-qwc5Q"
      },
      "source": [
        "### Create a sample file\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0yOpv-RouQW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e65f8576-e92c-4412-97ec-ae2911f9f7ed"
      },
      "source": [
        "# The default format is: \n",
        "# - tab separated\n",
        "# - column \"text\"\n",
        "# - column \"label\" \n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\"text\": [\"The concerts supercaliphractisch was great!\", \"I hate people ignoring climate change.\"],\n",
        "                  \"label\": [\"positive\",\"negative\"]\n",
        "                  })\n",
        "print(df)\n",
        "df.to_csv(\"train.tsv\", sep=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                          text     label\n",
            "0  The concerts supercaliphractisch was great!  positive\n",
            "1       I hate people ignoring climate change.  negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyaxKN5-woDP"
      },
      "source": [
        "### Investigate how the processor converts the file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zeTedwcsKyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "401f1ea1-1b82-4bbb-8c98-85c9f02153cf"
      },
      "source": [
        "tokenizer = Tokenizer.load(\n",
        "    pretrained_model_name_or_path=\"bert-base-uncased\")\n",
        "\n",
        "processor = TextClassificationProcessor(data_dir = \"\", \n",
        "                                        tokenizer=tokenizer,\n",
        "                                        max_seq_len=64,\n",
        "                                        label_list=[\"positive\",\"negative\"],\n",
        "                                        label_column_name=\"label\",\n",
        "                                        metric=\"acc\",\n",
        "                                       )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11/23/2020 19:02:22 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'BertTokenizer'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCFj24wVuNwD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f82746d-213b-47ac-e853-e3205e280d3d"
      },
      "source": [
        "#  1. One File -> Dictionarie(s) with \"raw data\"\n",
        "dicts = processor.file_to_dicts(file=\"train.tsv\")\n",
        "print(dicts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'text': 'The concerts supercaliphractisch was great!', 'text_classification_label': 'positive'}, {'text': 'I hate people ignoring climate change.', 'text_classification_label': 'negative'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6olb9V10WJXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2e07965-b8c6-4d80-9ab6-82f87e57e78b"
      },
      "source": [
        "#  2. One Dictionary -> Sample(s) \n",
        "#     (Sample = \"clear text\" model input + meta information) \n",
        "samples = processor._dict_to_samples(dictionary=dicts[0])\n",
        "\n",
        "# print each attribute of sample\n",
        "print(samples[0].clear_text)\n",
        "print(samples[0].tokenized)\n",
        "print(samples[0].features)\n",
        "print(\"----------------------------------\\n\\n\\n\")\n",
        "\n",
        "# or in a nicer, formatted style\n",
        "print(samples[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': 'The concerts supercaliphractisch was great!', 'text_classification_label': 'positive'}\n",
            "{'tokens': ['the', 'concerts', 'super', '##cal', '##ip', '##hra', '##ct', '##isch', 'was', 'great', '!'], 'offsets': [0, 4, 13, 18, 21, 23, 26, 28, 33, 37, 42], 'start_of_word': [True, True, True, False, False, False, False, False, True, True, False]}\n",
            "None\n",
            "----------------------------------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: None\n",
            "Clear Text: \n",
            " \ttext: The concerts supercaliphractisch was great!\n",
            " \ttext_classification_label: positive\n",
            "Tokenized: \n",
            " \ttokens: ['the', 'concerts', 'super', '##cal', '##ip', '##hra', '##ct', '##isch', 'was', 'great', '!']\n",
            " \toffsets: [0, 4, 13, 18, 21, 23, 26, 28, 33, 37, 42]\n",
            " \tstart_of_word: [True, True, True, False, False, False, False, False, True, True, False]\n",
            "Features: \n",
            " \tNone\n",
            "_____________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0tGhxydWK7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14fb0124-b2bf-4f18-bc9a-1c8f82f3b17f"
      },
      "source": [
        "# 3. One Sample -> Features\n",
        "#    (Features = \"vectorized\" model input)\n",
        "\n",
        "features = processor._sample_to_features(samples[0])\n",
        "print(features[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': [101, 1996, 6759, 3565, 9289, 11514, 13492, 6593, 19946, 2001, 2307, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'padding_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'segment_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'text_classification_label_ids': [0]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0NkQxIojDDV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnNWsUChr_Bc"
      },
      "source": [
        "# 2) Hands-On: Adjust it to your dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kjAyYmYeLAD"
      },
      "source": [
        "\n",
        "## Task 1: Use an existing Processor\n",
        "\n",
        "This works if you have:\n",
        "- standard tasks\n",
        "- common file formats \n",
        "\n",
        "**Example: Text classification on CSV with multiple columns**\n",
        "\n",
        "Dataset: GermEval18 (Hatespeech detection)  \n",
        "Format: TSV  \n",
        "Columns: `text coarse_label fine_label`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "819nNLiGqBXY"
      },
      "source": [
        "# Download dataset\n",
        "from farm.data_handler import utils\n",
        "utils._download_extract_downstream_data(\"germeval18/train.tsv\")\n",
        "!head -n 10 germeval18/train.tsv\n",
        "\n",
        "# Task: Initialize a processor for the above file by passing the right arguments\n",
        "\n",
        "processor = TextClassificationProcessor(tokenizer=tokenizer,\n",
        "                                        max_seq_len=128,\n",
        "                                        data_dir= ???,\n",
        "                                        train_filename=???,\n",
        "                                        label_list=???,\n",
        "                                        metric=\"acc\",\n",
        "                                        label_column_name=???\n",
        "                                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGG8wUtKwRvc"
      },
      "source": [
        "# test it\n",
        "dicts = processor.file_to_dicts(file=\"germeval18/train.tsv\")\n",
        "print(dicts[0])\n",
        "assert dicts[0] == {'text': '@corinnamilborn Liebe Corinna, wir würden dich gerne als Moderatorin für uns gewinnen! Wärst du begeisterbar?', 'text_classification_label': 'OTHER'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtcCU9dOeWtf"
      },
      "source": [
        "## Task 2: Build your own Processor\n",
        "This works best for:\n",
        "- custom input files\n",
        "- special preprocessing steps\n",
        "- advanced multitask learning \n",
        "\n",
        "**Example: Text classification with JSON as input file** \n",
        "\n",
        "Dataset: [100k Yelp reviews](https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-downstream/yelp_reviews_100k.json) ( [full dataset](https://https://www.yelp.com/dataset/download), [documentation](https://https://www.yelp.com/dataset/documentation/main))\n",
        "\n",
        "Format: \n",
        "\n",
        "``` \n",
        "{\n",
        "...\n",
        "    // integer, star rating\n",
        "    \"stars\": 4,\n",
        "\n",
        "    // string, the review itself\n",
        "    \"text\": \"Great place to hang out after work: the prices are decent, and the ambience is fun. It's a bit loud, but very lively. The staff is friendly, and the food is good. They have a good selection of drinks.\",\n",
        "...\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV5DqqTp361t"
      },
      "source": [
        "# Download dataset\n",
        "!wget https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-downstream/yelp_reviews_100k.json\n",
        "!head -5 yelp_reviews_100k.json\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY62TbxjqB-i"
      },
      "source": [
        "# Task: Create a new TextClassificationProcessor that reads in json instead of csv/tsv.\n",
        "# You can simply overwrite the function that reads from the file\n",
        "\n",
        "class CustomTextClassificationProcessor(TextClassificationProcessor):\n",
        "  \n",
        "    # we need to overwrite this function from the parent class,\n",
        "    # because we read a json instead of a tsv/csv.\n",
        "    def file_to_dicts(self, file: str) -> [dict]:\n",
        "      \n",
        "      #TODO: your turn :)\n",
        "\n",
        "      # The returned list of dicts should look like this:\n",
        "      # [{'text': 'Total bill for this horrible service? ...',\n",
        "      #   'text_classification_label': '4'}, ...]\n",
        "      return dicts\n",
        "    \n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCI3Ed6WwXNy"
      },
      "source": [
        "processor = CustomTextClassificationProcessor(tokenizer=tokenizer,\n",
        "                                              max_seq_len=128,\n",
        "                                              data_dir=\"\",\n",
        "                                              label_list=[\"1\",\"2\",\"3\",\"4\",\"5\"],\n",
        "                                              metric=\"acc\",\n",
        "                                              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOEnt_3ht9yy"
      },
      "source": [
        "# test it\n",
        "\n",
        "dicts = processor.file_to_dicts(file=\"yelp_reviews_100k.json\")\n",
        "print(dicts[0])\n",
        "\n",
        "assert dicts[0] == {'text': 'Total bill for this horrible service? Over $8Gs. These crooks actually had the nerve to charge us $69 for 3 pills. I checked online the pills can be had for 19 cents EACH! Avoid Hospital ERs at all costs.', 'text_classification_label': '1'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrzRAtL7PUcD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}